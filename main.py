import json
import os
import time
from utils import (
    extract_frames,
    run_ocr,
    denoise_ocr,
    merge_text_across_frames_for_understanding,
    build_timeline,
    build_prompt
)
from llm_client import LLMClient

# æœ€ç»ˆå®ç°
if __name__ == "__main__":
    start = time.perf_counter()
    video_path = "sample_videos/ä½“è‚²æ–°é—»çƒ­ç‚¹.mp4"
    frame_dir = "frames"
    #OCR æå–å¹¶ä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼

    interval_sec=5#æŠ½å¸§é—´éš”
    frames = extract_frames(video_path, frame_dir, interval_sec)
    ocr_raw = run_ocr(frames)
    ocr_cleaned = denoise_ocr(ocr_raw, conf_threshold=0.75)
    final_segments = merge_text_across_frames_for_understanding(ocr_cleaned,sim_threshold=0.92,time_gap_merge=interval_sec + 1 )
    timeline_text = build_timeline(final_segments)
        
    default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ­è§†é¢‘å†…å®¹ç†è§£ä¸å®¡æ ¸æ¨¡å‹ã€‚
    ä½ å°†åŸºäºè§†é¢‘ä¸­é€šè¿‡ OCR æå–çš„æ–‡å­—å†…å®¹ï¼Œå¯¹è§†é¢‘è¿›è¡Œå¤šç»´åº¦åˆ†æã€‚

    ã€èƒŒæ™¯è¯´æ˜ã€‘
    - ä»¥ä¸‹æ–‡å­—æŒ‰æ—¶é—´é¡ºåºæå–è‡ªè§†é¢‘ç”»é¢ï¼ˆåŒ…æ‹¬å­—å¹•ã€æ ‡é¢˜ã€æ°´å°ç­‰ï¼‰ã€‚
    - è‹¥æŸæ–‡æœ¬åœ¨è¿ç»­æ—¶é—´æ®µé‡å¤å‡ºç°ï¼Œé€šå¸¸è¡¨ç¤ºå…¶ä¸ºæ ¸å¿ƒä¿¡æ¯æˆ–å›ºå®šæ ‡è¯†ã€‚
    - æ–‡æœ¬å¯èƒ½åŒ…å«å£è¯­åŒ–è¡¨è¾¾ã€è¥é”€è¯æœ¯æˆ–ä¸å®Œæ•´å¥å­ï¼Œè¯·ç»“åˆæ•´ä½“è¯­å¢ƒç†è§£ã€‚

    ã€è§†é¢‘æ–‡å­—æ—¶é—´è½´ã€‘
    {timeline_text}

    ã€åˆ†æä»»åŠ¡ã€‘
    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ 8 é¡¹å®Œæˆåˆ†æï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ï¼š

    1. summary: ç”¨ä¸€å¥è¯æ¦‚æ‹¬è§†é¢‘ä¸»è¦å†…å®¹
    2. tags: ç»™å‡º 3~5 ä¸ªå†…å®¹æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    3. category: å†…å®¹ç±»å‹ï¼ˆå¦‚ æ–°é—»ã€ä½“è‚²ã€å¨±ä¹ã€å¹¿å‘Š ç­‰ï¼‰
    4. genre: å†…å®¹ä½“è£ï¼ˆå¦‚ èµ›äº‹æŠ¥é“ã€äººç‰©ç‰¹å†™ã€å¿«è®¯ ç­‰ï¼‰
    5. tone: æ•´ä½“è°ƒæ€§ï¼ˆå¦‚ å®¢è§‚ã€ç…½æƒ…ã€å¹½é»˜ã€ä¸¥è‚ƒ ç­‰ï¼‰
    6. sentiment: æƒ…æ„Ÿå€¾å‘ï¼ˆå¦‚ ç§¯æã€æ¶ˆæã€ä¸­æ€§ï¼‰
    7. is_low_quality: æ˜¯å¦ä¸ºä½è´¨å†…å®¹ï¼ˆæ˜¯/å¦ï¼‰
    8. has_risk: æ˜¯å¦å­˜åœ¨æ½œåœ¨è¿è§„é£é™©ï¼ˆæ˜¯/å¦ï¼‰

    ã€è¾“å‡ºæ ¼å¼ã€‘
    åªè¾“å‡ºä¸€ä¸ªåˆæ³• JSON å¯¹è±¡ï¼Œå­—æ®µåå¿…é¡»ä¸ºä¸Šè¿°è‹±æ–‡åã€‚"""

    prompt = build_prompt(default_prompt,timeline_text=timeline_text)

    llm = LLMClient(
        api_key=os.getenv('DASHSCOPE_API_KEY'),
        api_url="https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
        model_name="qwen-plus"  # ç¤ºä¾‹
    )
    try:
        analysis_result = llm.analyze(prompt)
        print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
        
        
    except Exception as e:
        print("åˆ†æå¤±è´¥:", str(e))
    elapsed = time.perf_counter() - start
    print(f"\nğŸ•’ æ•´ä¸ªæµç¨‹è€—æ—¶: {elapsed:.2f} ç§’")
